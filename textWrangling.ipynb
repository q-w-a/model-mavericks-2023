{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Team Model Mavericks: Text Wrangling Part of the event"
      ],
      "metadata": {
        "id": "5Hhd0b0lRUC4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDdf0bNSR4Eo"
      },
      "outputs": [],
      "source": [
        "# For Google Colab connected to a Google Drive Only\n",
        "# data will be deleted from google drive after \n",
        "# Model Mavericks: Kush, Nikki, Quinn, Clara, Rose\n",
        "! cd ..\n",
        "! cp \"/content/drive/MyDrive/Colab Notebooks/dataFest.zip\" \"/content/sample_data/\"\n",
        "!unzip /content/sample_data/dataFest.zip >> out.txt\n",
        "!mv \"/content/DataFest 2023 Data For Distribution/data\" \"/\"\n",
        "!sudo apt install neofetch >> out.txt\n",
        "!ls -la /data\n",
        "!neofetch --stdout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from collections import Counter\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "zThUAQCfWVyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>"
      ],
      "metadata": {
        "id": "5AAY2SkcWpno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Todo: change the relevant portions if copying to a new Jupyter Notebook\n",
        "# Reading in all the (relevant) CSVs\n",
        "attorneys = pd.read_csv('/data/attorneys.csv')\n",
        "attorneytimeentries = pd.read_csv('/data/attorneytimeentries.csv')\n",
        "clients = pd.read_csv('/data/clients.csv')\n",
        "categories = pd.read_csv('/data/categories.csv')\n",
        "questions = pd.read_csv('/data/questions.csv')\n",
        "#questionposts = pd.read_csv('/data/questionposts.csv')\n",
        "statesites = pd.read_csv('/data/statesites.csv')\n",
        "subcategories = pd.read_csv('/data/subcategories.csv')"
      ],
      "metadata": {
        "id": "k_uLfjhtWoNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/data/questionposts.csv', 'rb') as f:\n",
        "    data = f.read()\n",
        "    # Replace any NULL bytes with the string \"<NULL>\"\n",
        "    data = data.replace(b'\\x00', b'<NULL>')\n",
        "    # Decode the data using the utf-8 encoding\n",
        "    decoded_data = data.decode('utf-8')\n",
        "\n",
        "# Now you can parse the CSV data as before\n",
        "reader = csv.reader(decoded_data.splitlines(), delimiter=',', quotechar='\"')\n",
        "rows = []\n",
        "for cols in reader:\n",
        "    # Extract the ID, state, question ID, and date fields\n",
        "    if len(cols) < 4:\n",
        "        continue\n",
        "    id_num = cols[0]\n",
        "    state = cols[1]\n",
        "    question_id = cols[2]\n",
        "    post_text = ','.join(cols[3:-1]).replace('<NULL>', '\\x00')\n",
        "    created_utc = cols[-1]\n",
        "    # Append the row data as a tuple to the list of rows\n",
        "    rows.append((id_num, state, question_id, post_text, created_utc))\n",
        "\n",
        "# Create a dataframe from the list of rows\n",
        "questionposts = pd.DataFrame(rows, columns=['ID', 'StateAbbr', 'QuestionUno', 'PostText', 'CreatedUtc'])\n",
        "questionposts = questionposts.drop(0) # The first row is literally just names of columns\n",
        "questionposts = questionposts.reset_index(drop=True) # Do not need the first index column\n",
        "questionposts = questionposts.set_index(\"ID\") # ID is the index, so resetting back to it"
      ],
      "metadata": {
        "id": "KErpL5csrVrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questionposts, questionposts.columns, questions.columns"
      ],
      "metadata": {
        "id": "IpITtU7nmx-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_expanded_questions = pd.merge(questions, questionposts, on=\"QuestionUno\")\n",
        "merged_expanded_questions = merged_expanded_questions.rename(columns={'StateAbbr_x': 'StateAbbr'})\n",
        "merged_expanded_questions = merged_expanded_questions.drop('StateAbbr_y', axis=1)"
      ],
      "metadata": {
        "id": "6Ax_yiTty1jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_expanded_questions.columns"
      ],
      "metadata": {
        "id": "5kUPNIG8zF5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_expanded_questions"
      ],
      "metadata": {
        "id": "QKg1r6JgzH4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding Top Unique Questions Per Category \n",
        "# By question ID\n",
        "# Group the dataframe by category and question ID and count the occurrences\n",
        "question_count = merged_expanded_questions.groupby(['Category', 'QuestionUno']).size().reset_index(name='Count')\n",
        "\n",
        "# Sort the dataframe by count in descending order\n",
        "question_count = question_count.sort_values(by=['Category', 'Count'], ascending=[True, False])\n",
        "\n",
        "# Display the top 10 questions per category\n",
        "for category in question_count['Category'].unique():\n",
        "    print(f'\\nTop 10 questions in {category}:')\n",
        "    print(question_count[question_count['Category']==category].head(10))"
      ],
      "metadata": {
        "id": "36FbdwMI3RlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by category and subcategory\n",
        "grouped = merged_expanded_questions.groupby(['Category', 'Subcategory'])\n",
        "\n",
        "# Initialize the list of stopwords\n",
        "stop_words = stopwords.words('english') + list(string.punctuation) + ['#']\n",
        "stemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "_v9T2xkG_O8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by state abbreviation, category, and subcategory\n",
        "grouped = merged_expanded_questions.groupby(['StateAbbr', 'Category', 'Subcategory'])\n",
        "\n",
        "# Count the number of questions in each group\n",
        "counts = grouped.size()\n",
        "\n",
        "# Convert the counts series to a dataframe and reset the index\n",
        "counts_df = counts.to_frame(name='Question Count').reset_index()\n",
        "\n",
        "# Sort the dataframe by state abbreviation and question count\n",
        "highest_counts = counts_df.sort_values(['StateAbbr', 'Question Count'], ascending=[True, False])\n",
        "\n",
        "# Keep only the highest counts for each state abbreviation\n",
        "highest_counts = highest_counts.groupby('StateAbbr').head(3)"
      ],
      "metadata": {
        "id": "GrG2oxP0qp-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#highest_categories = highest_categories.sort_values(by = \"QuestionUno\", ascending = False)\n",
        "highest_categories"
      ],
      "metadata": {
        "id": "wVQFMMWto5Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = highest_categories.drop('Subcategory', axis=1)\n",
        "merged_df = highest_categories.groupby(['StateAbbr', 'Category'])['QuestionUno'].sum().reset_index()\n",
        "\n",
        "# drop the SubCategory column\n",
        "\n",
        "\n",
        "# sort the dataframe by StateAbbr and Category\n",
        "merged_df = merged_df.sort_values(['StateAbbr', 'Category'])\n",
        "\n",
        "# reset the index\n",
        "merged_df = merged_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "b1C33kZ2hx2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_summed = merged_df.groupby(['StateAbbr', 'Category'])['QuestionUno'].sum().reset_index()\n",
        "merged_df_summed"
      ],
      "metadata": {
        "id": "j73w1gtkh9cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df_summed.to_csv(\"Highest_Categories_No_Subcategories.csv\")"
      ],
      "metadata": {
        "id": "V1_VVr1gjP4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highest_categories.to_csv(\"Highest Categories\")"
      ],
      "metadata": {
        "id": "MrpZZ4libJGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = merged_expanded_questions\n",
        "train, test = train_test_split(df, test_size=0.005)"
      ],
      "metadata": {
        "id": "Mkpf22v77jBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataframe\n",
        "df = test\n",
        "\n",
        "# Define the stemmer and stopwords\n",
        "stemmer = SnowballStemmer('english')\n",
        "#stop_words = stopwords.words('english')\n",
        "stop_words += ['thank', 'thanks', 'greetings']\n",
        "\n",
        "# Create an empty dataframe to store the results\n",
        "results = pd.DataFrame(columns=['State', 'Category', 'Top Terms', 'Top Questions'])\n",
        "\n",
        "# Group by state, category, and subcategory\n",
        "grouped = df.groupby(['StateAbbr', 'Category', 'Subcategory'])\n",
        "\n",
        "# Loop through each group\n",
        "for name, group in grouped:\n",
        "    state = name[0]\n",
        "    category = name[1] + ' - ' + name[2]\n",
        "\n",
        "    # Tokenize the questions and remove stopwords\n",
        "    tokens = [word.lower() for sentence in group['PostText'] for word in word_tokenize(sentence) if word.lower() not in stop_words and not word.isdigit() and len(word) > 1]\n",
        "\n",
        "    # Count the frequency of each term or phrase\n",
        "    frequency = Counter(tokens)\n",
        "\n",
        "    # Get the top 10 terms\n",
        "    top_terms = [t for t, f in frequency.most_common(10)]\n",
        "\n",
        "    # Get the top 10 questions\n",
        "    top_questions = []\n",
        "    for post in group['PostText']:\n",
        "        sentences = sent_tokenize(post)\n",
        "        for sentence in sentences:\n",
        "            if any(q.lower() in sentence.lower() for q in top_terms):\n",
        "                top_questions.append(sentence.strip())\n",
        "\n",
        "    # Add the results to the dataframe\n",
        "    results = results.append({'State': state, 'Category': category, 'Top Terms': ', '.join(top_terms), 'Top Questions': '\\n\\n'.join(top_questions)}, ignore_index=True)\n",
        "\n",
        "# Sort the results by state and question frequency\n",
        "results = results.sort_values(by=['State', 'Top Questions'], ascending=[True, False])"
      ],
      "metadata": {
        "id": "sH2BiK4aVogs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "g4oNBKuDWT2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataframe\n",
        "df = test\n",
        "\n",
        "# Initialize the stemmer and stopwords\n",
        "#stemmer = SnowballStemmer('english')\n",
        "#stop_words = stopwords.words('english')\n",
        "\n",
        "# Create a new dataframe to store the results\n",
        "results_df = pd.DataFrame(columns=['State', 'Category', 'Top Terms', 'Top Questions'])\n",
        "\n",
        "# Group by state and category\n",
        "grouped = df.groupby(['StateAbbr', 'Category'])\n",
        "\n",
        "# Loop through each group\n",
        "for name, group in grouped:\n",
        "    state = name[0]\n",
        "    category = name[1]\n",
        "    \n",
        "    # Combine all the posts into one string\n",
        "    posts = ' '.join(group['PostText'].tolist())\n",
        "    \n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(posts)\n",
        "    \n",
        "    # Tokenize, stop, stem, and count the words in each sentence\n",
        "    words = []\n",
        "    for sentence in sentences:\n",
        "        sentence_words = [stemmer.stem(word.lower()) for word in word_tokenize(sentence) if word.lower() not in stop_words and word.isalpha()]\n",
        "        words.extend(sentence_words)\n",
        "    \n",
        "    # Count the frequency of each term\n",
        "    frequency = Counter(words)\n",
        "    \n",
        "    # Get the top 10 terms\n",
        "    top_terms = ', '.join([word for word, count in frequency.most_common(10)])\n",
        "    \n",
        "    # Get the top 10 questions\n",
        "    questions = []\n",
        "    for sentence in sentences:\n",
        "        if '?' in sentence:\n",
        "            sentence_questions = [question.strip() for question in sentence.split('?') if question.strip()]\n",
        "            questions.extend(sentence_questions)\n",
        "    top_questions = '\\n'.join([question for question, count in Counter(questions).most_common(10)])\n",
        "    \n",
        "    # Add the results to the dataframe\n",
        "    results_df = results_df.append({'State': state, 'Category': category, 'Top Terms': top_terms, 'Top Questions': top_questions}, ignore_index=True)\n",
        "\n",
        "# Merge subcategories within each category\n",
        "results_df['Category'] = results_df['Category'].str.split(' - ').str[0]\n",
        "results_df = results_df.groupby(['State', 'Category']).agg({'Top Terms': ', '.join, 'Top Questions': '\\n'.join}).reset_index()"
      ],
      "metadata": {
        "id": "Jm0xV1wMWbJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataframe\n",
        "df = test\n",
        "\n",
        "# Tokenize and stem the text, and remove stop words and non-alpha tokens\n",
        "def tokenize(text):\n",
        "    tokens = [stemmer.stem(token.lower()) for token in word_tokenize(text) if token.isalpha() and token.lower() not in stop_words and len(token) > 1]\n",
        "    return tokens\n",
        "\n",
        "# Initialize the result dataframe\n",
        "result = pd.DataFrame(columns=['State', 'Category', 'Top Terms', 'Top 3 Questions'])\n",
        "\n",
        "# Loop through each state\n",
        "for state in df['StateAbbr'].unique():\n",
        "    # Filter the dataframe by state\n",
        "    state_df = df[df['StateAbbr'] == state]\n",
        "    \n",
        "    # Loop through each category\n",
        "    for category in state_df['Category'].unique():\n",
        "        # Filter the dataframe by category\n",
        "        category_df = state_df[state_df['Category'] == category]\n",
        "        \n",
        "        # Tokenize and stem the text\n",
        "        tokens = [tokenize(text) for text in category_df['PostText']]\n",
        "        \n",
        "        # Flatten the list of tokens\n",
        "        flat_tokens = [token for sublist in tokens for token in sublist]\n",
        "        \n",
        "        # Count the frequency of each token\n",
        "        term_frequency = Counter(flat_tokens)\n",
        "        \n",
        "        # Get the top 10 terms\n",
        "        top_terms = [term for term, count in term_frequency.most_common(10)]\n",
        "        \n",
        "        # Get the top 3 questions\n",
        "        questions = []\n",
        "        for text in category_df['PostText']:\n",
        "            # Tokenize the text into sentences\n",
        "            sentences = sent_tokenize(text)\n",
        "            \n",
        "            # Loop through each sentence\n",
        "            for sentence in sentences:\n",
        "                # Check if the sentence ends with a question mark\n",
        "                if sentence.endswith('?'):\n",
        "                    # Remove leading and trailing white space\n",
        "                    sentence = sentence.strip()\n",
        "                    \n",
        "                    # Append the sentence to the list of questions\n",
        "                    questions.append(sentence)\n",
        "                    \n",
        "                    # Break out of the loop if we have found 3 questions\n",
        "                    if len(questions) == 5:\n",
        "                        break\n",
        "                        \n",
        "            # Break out of the loop if we have found 3 questions\n",
        "            if len(questions) == 5:\n",
        "                break\n",
        "        \n",
        "        # Add the results to the result dataframe\n",
        "        result = result.append({\n",
        "            'State': state,\n",
        "            'Category': category,\n",
        "            'Top Terms': ', '.join(top_terms),\n",
        "            'Top Question': '\\n'.join(questions)\n",
        "        }, ignore_index=True)\n",
        "\n",
        "# Print the result dataframe"
      ],
      "metadata": {
        "id": "Qm-ipktQY_JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_sorted = result.sort_values(by=\"State\", ascending=True)"
      ],
      "metadata": {
        "id": "oO_1A3LOc_5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_sorted.to_csv(\"Top_Question_Categorically.csv\")"
      ],
      "metadata": {
        "id": "CyekGR-Ljsmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}